{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849e75d3",
   "metadata": {},
   "source": [
    "# Sleep Disorder Risk Prediction — Data Prep & Baseline Model\n",
    "\n",
    "This notebook runs the end-to-end data preparation, baseline training, evaluation, and model export steps. **Do not run it without first installing requirements in `requirements.txt`.**\n",
    "\n",
    "**High-level steps:**\n",
    "1. Download NHANES cycles (2005–2016) and convert XPT → CSV (uses `src/check_nhanes_downloads.py`).\n",
    "2. Merge converted CSV files and engineer features (uses `src/merge_and_engineer.py`).\n",
    "3. Load the merged CSV, preprocess, split, scale, train a baseline XGBoost model, evaluate metrics, and compute SHAP explanations.\n",
    "\n",
    "Run cells sequentially. If you prefer to run parts separately, run the downloader and merge scripts from the terminal first, then continue with the notebook cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110d9fa",
   "metadata": {},
   "source": [
    "## 0. Environment & Requirements\n",
    "\n",
    "Install dependencies from `requirements.txt` (recommended to use a virtual environment):\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "This notebook expects the following folders after running the downloader:\n",
    "- `data/raw/<cycle>/csv/` (converted CSVs)\n",
    "- `reports/` (per-cycle JSON reports)\n",
    "- After merging: `data/processed/merged_clean.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) (Optional) Run the downloader from the notebook.\n",
    "# Running this cell will download ~10+ files per cycle and may take a long time depending on your connection.\n",
    "# If you prefer, run in a terminal: python src/check_nhanes_downloads.py --out data/raw --report reports --convert\n",
    "\n",
    "# !python src/check_nhanes_downloads.py --out data/raw --report reports --convert\n",
    "\n",
    "print('Downloader command (commented out). To run, remove comment and execute this cell or run from terminal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b79142",
   "metadata": {},
   "source": [
    "## 2) Inspect per-cycle reports\n",
    "The downloader writes `reports/<cycle>_report.json` describing which files were downloaded and which variables were present/missing. Always inspect these reports before merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "report_dir = Path('reports')\n",
    "if report_dir.exists():\n",
    "    for p in sorted(report_dir.glob('*_report.json')):\n",
    "        print('\\nReport:', p)\n",
    "        with open(p, 'r', encoding='utf-8') as fh:\n",
    "            rep = json.load(fh)\n",
    "        # Print summary of missing variables per file\n",
    "        for fname, info in rep.get('files', {}).items():\n",
    "            missing = info.get('missing_variables')\n",
    "            if missing:\n",
    "                print(f\"  {fname}: missing {len(missing)} key vars -> {missing}\")\n",
    "            else:\n",
    "                print(f\"  {fname}: OK\")\n",
    "else:\n",
    "    print('No reports found. Run the downloader first (see instructions).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8b900",
   "metadata": {},
   "source": [
    "## 3) Merge & engineer features\n",
    "You can run the merge script from the notebook or via terminal. This script reads CSVs from `data/raw/<cycle>/csv/`, harmonizes, merges on `SEQN`, engineers features, and writes `data/processed/merged_clean.csv`. If variable names differ across cycles you may need to edit `src/merge_and_engineer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2511587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run merge script (uncomment to execute here)\n",
    "# !python src/merge_and_engineer.py --input data/raw --output data/processed/merged_clean.csv\n",
    "print('Merge command (commented out). Run from terminal or uncomment to run here.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba18de8",
   "metadata": {},
   "source": [
    "## 4) Load merged data and inspect\n",
    "Load `data/processed/merged_clean.csv` and inspect label balance and missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ea5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "p = Path('data/processed/merged_clean.csv')\n",
    "if not p.exists():\n",
    "    print('Merged CSV not found. Run merge_and_engineer.py first.')\n",
    "else:\n",
    "    df = pd.read_csv(p)\n",
    "    print('Merged shape:', df.shape)\n",
    "    print('\\nTarget distribution (sleep_disorder):')\n",
    "    print(df['sleep_disorder'].value_counts(dropna=False))\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0cea81",
   "metadata": {},
   "source": [
    "## 5) Preprocess, cross-validation split, scaling, and baseline training\n",
    "This cell runs preprocessing: drop NA for core features, split into train/test (stratified), scale numeric features, and train a baseline XGBoost. Adjust hyperparameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline training (example) - DO NOT run until merged CSV is present\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, confusion_matrix\n",
    "\n",
    "if 'df' not in globals():\n",
    "    print('Load merged CSV first (see previous cell).')\n",
    "else:\n",
    "    feature_cols = [\n",
    "        'age', 'sex', 'BMI', 'exercise_min_week',\n",
    "        'calories_day', 'fiber_g_day', 'added_sugar_g_day', 'caffeine_mg_day',\n",
    "        'alcohol_drinks_week', 'current_smoker', 'depression_score',\n",
    "        'systolic_bp', 'diastolic_bp'\n",
    "    ]\n",
    "    # Keep only rows where target is not null\n",
    "    df2 = df[df['sleep_disorder'].notna()].copy()\n",
    "    # Drop rows missing core features\n",
    "    df_model = df2[feature_cols + ['sleep_disorder']].dropna()\n",
    "    print('After dropping missing:', df_model.shape)\n",
    "    X = df_model[feature_cols]\n",
    "    y = df_model['sleep_disorder'].astype(int)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    # Train XGBoost baseline\n",
    "    try:\n",
    "        from xgboost import XGBClassifier\n",
    "        model = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')\n",
    "        model.fit(X_train_s, y_train)\n",
    "        y_proba = model.predict_proba(X_test_s)[:,1]\n",
    "        y_pred = model.predict(X_test_s)\n",
    "        roc = roc_auc_score(y_test, y_proba)\n",
    "        pr = average_precision_score(y_test, y_proba)\n",
    "        print(f'ROC-AUC: {roc:.3f} | PR-AUC: {pr:.3f}')\n",
    "        print('\\nClassification report:')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    except Exception as e:\n",
    "        print('Error training model (is xgboost installed?):', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f06ad",
   "metadata": {},
   "source": [
    "## 6) SHAP explainability (global & local)\n",
    "Use SHAP to compute feature importances and visualize per-person explanations. This cell shows example code — it may be slow on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84861d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP example (requires shap and matplotlib)\n",
    "if 'model' in globals() and model is not None:\n",
    "    try:\n",
    "        import shap\n",
    "        import matplotlib.pyplot as plt\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        # Use a small sample to compute shap values for speed\n",
    "        sample = X_test_s[:200]\n",
    "        shap_values = explainer.shap_values(sample)\n",
    "        shap.summary_plot(shap_values, sample, feature_names=feature_cols)\n",
    "    except Exception as e:\n",
    "        print('Error computing SHAP (install shap and matplotlib):', e)\n",
    "else:\n",
    "    print('Train the model first (run the training cell above).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d57dfe",
   "metadata": {},
   "source": [
    "## 7) Save model & scaler\n",
    "Save trained artifacts to `models/` for deployment with Streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba416d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "if 'model' in globals() and 'scaler' in globals():\n",
    "    joblib.dump(model, 'models/sleep_risk_model.pkl')\n",
    "    joblib.dump(scaler, 'models/feature_scaler.pkl')\n",
    "    joblib.dump(feature_cols, 'models/feature_names.pkl')\n",
    "    print('Saved model and scaler to models/')\n",
    "else:\n",
    "    print('No model/scaler found in memory. Run training cell first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3cdfc",
   "metadata": {},
   "source": [
    "## Next steps & notes\n",
    "- Inspect reports for variable coverage per cycle and edit `src/merge_and_engineer.py` if variable names differ across cycles.\n",
    "- Consider multiple imputation (MICE) for missingness instead of complete-case analysis.\n",
    "- Use temporal external validation: train on early cycles and test on later cycles to assess generalization.\n",
    "- Tune hyperparameters with `GridSearchCV` or `Optuna` and log experiments with MLflow or Weights & Biases.\n",
    "\n",
    "**Limitations:** See README for full limitations and ethical notes.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
